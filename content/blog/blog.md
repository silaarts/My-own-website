---
title: "Artificial Intelligence in the media: who's to blame?"
author: "Sil Aarts"
publishDate: "2019-10-17"

---

##### "Is Artifical Intelligence good?" [@LaForge_Tech](https://twitter.com/LaForge_Tech/status/1183755514758475776), October 14, 2019.

##### “If we don't act, #ArtificialIntelligence will impact #HumanRights. [...] [@NLatUNESCO](https://twitter.com/NLatUNESCO/status/1183682723350794245), October 14, 2019.  

It seems to have become an international hobby: putting the blame on Artificial Intelligence (AI). These kinds of phrases used by and in the media to describe AI make it seem as if computers and robots are taking over our world as we speak. Phrases such as *"Artificial Intelligence is biased"* are the rule rather than the exception. But why blame AI?

{{< figure src="/post/images/hoofd.png" >}}

In AI, a distinction is made between *narrow AI* and *general AI*. We refer to narrow AI, also titled weak AI, when we refer to technology that is already present in the world around us. Consider the "specially selected" section of Netflix. Based on the movies you have already seen, Netflix makes predictions about movies you might also like. Supermarkets that offer discounts on products based on their expiration date (the shorter the shelf life, the higher the discount)? Indeed, also AI.

*Machine Learning* (ML) is an important area of AI. ML involves the development of algorithms to *"allow machines to learn"*. Algorithms are sequences of instructions aimed at solving a problem. Algorithms are used, among other things, to make predictions regarding the weather or disease occurrences. When perceiving an algorithm like a sequence of instructions (i.e. if > then constructions), a lot of things can be viewed as an algorithm. Going to run your first 5k? Make it an algorithm. Adhere to your training schedule > get enough sleep and eat healthy meals > decrease the amount of K’s in your race week > eat a proper meal at least 2 hours before the race > warm-up > GO.

When it involves algorithms the following often applies: the more data, the more accurate the predictions. If considering Netflix, this means that if you've seen all romcoms with your favorite actress Jennifer Aniston, the algorithm probably will recommend a romcom with Sandra Bullock, but not that well-known horror movie with that scary clown. The prediction of the Netflix algorithm is aimed at making predictions as accurate as possible. The better the prediction, the greater the chance that you actually watch the recommended movie (and hopefully like it), the smaller the chance that you cancel your Netflix subscription and the greater the chance that you recommend Netflix. 

General AI, also known as strong AI, is what Hollywood shows us (I bet that now the words *“I'll be back”* pop into your head): robots that (who?) can think like humans and have consciousness. It is this form of AI that people have in mind when talking about robots taking over the world. However, this form of technology does not exist. 

In contrast to what the term "machine learning" suggests, computers, and therefore algorithms, do not learn like people. Computers that "learn" are in no way similar to human brains: there is no general AI. Computers, in contrast to people, and contrary to what is often assumed, have no intelligence. At least, not the form of intelligence that we humans have. Computers rely on human input and directions. Consequently, computers can only be as good as the information we, humans, provide them with. If we provide computers with biased information, biased information will come out: garbage in > garbage out.

An example: a detection system that determines whether a face is present in, let’s say, a large crowd. This AI system must be able to adequately identify every single face in order to, for example, be able to distinguish between someone who is searched for by the police and someone who is not. Hence, this AI system is trained on thousands, maybe even millions of faces and thereby ‘learns to distinguish one face from the other’. But what if the millions of faces on which this AI system is trained, primarily contain faces of white male people? In practice it appears that some of these systems disadvantage certain demographic groups: people with a dark skin color (especially females) are more often misidentified compared to white people (click [here](https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html) for more information). In other words: a person with a dark skin color has a greater chance of being mistaken for someone else than a person with a lighter skin color. This may not seem very problematic until you are arrested because you are being mistaken for a person who has committed a robbery. So far, decisions based on AI systems have been made in the area of mortgages, jobs and even [imprisonment](https://www.technologyreview.com/s/612775/algorithms-criminal-justice-ai/). With all its consequences.

The question that rises is who’s to blame if such algorithms lead to discriminatory and false results. If we have to believe the wording used in and by the media, AI is responsible. However, data is collected and stored by humans; algorithms are developed by humans. If the use of algorithms, and therefore AI, can have negative consequences, but have been developed by humans, then who's really to blame? Computers that have no intelligence and consciousness or people who have?

Why am I making such a fuzz about the choice of words media use to address AI, you ask? Well, 
phrases in the media such as the ones above feed people's fear of AI. But wait a minute. You just said that the use of AI can actually do harm. It certainly can. But one can also inflict harm with a knife. And we all have those stacked in our kitchens. If the use of AI results in discriminative and catastrophic decisions, then this is not the fault of AI, but of mankind. Ever heard the media postulate phrases such as *“We must be scared of knifes”* or *“Are knives good?”*, when a knife was used in an armed robbery? Right!

While we, humans, are developing and deploying these algorithms, we are shifting the blame on computers, on AI, on these algorithms. However, as humans, we must recognize and acknowledge our role in AI and act accordingly. So, let's start with using words and phrases in and by the media that reflect the truth: if something goes wrong while using AI, we humans are to blame. Computers, Artificial Intelligence, Machine Learning, algorithms or whatever terms we use, can’t think and can’t make decisions. People can. We are responsible. At least, up until now …
